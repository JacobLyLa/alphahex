{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from hex import HexGame\n",
    "from neuralnet import createModel, loadModel\n",
    "from player import NeuralNetPlayer, RandomPlayer, MCTSPlayer, NeuralMCTSPlayer\n",
    "from tournament import Tournament\n",
    "import pickle\n",
    "from mcts import Mcts\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2450 games from replay buffer\n",
      "Baseline accuracy: 0.04122448979591837 with most common move 16\n"
     ]
    }
   ],
   "source": [
    "boardSize = 7\n",
    "dataName = f'replayBuffer{boardSize}.pickle'\n",
    "with open(dataName, \"rb\") as f:\n",
    "    replay = pickle.load(f)\n",
    "print(\"Loaded\", len(replay), \"games from replay buffer\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.array([x[0] for x in replay]).reshape(len(replay), -1)\n",
    "y = np.array([x[1] for x in replay]).reshape(len(replay), -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# calculate baseline accuracy by predicting the most common move on validation set\n",
    "from collections import Counter\n",
    "mostCommonMove = Counter([np.argmax(x[1]) for x in replay]).most_common(1)[0][0]\n",
    "baselineAccuracy = sum([np.argmax(x[1]) == mostCommonMove for x in replay])/len(replay)\n",
    "print(\"Baseline accuracy:\", baselineAccuracy, \"with most common move\", mostCommonMove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "c:\\Users\\Jacob\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.1755\n",
      "Best validation loss: 3.3929\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "from keras.layers import ELU\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import ELU\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Conv2D, Flatten\n",
    "\n",
    "from keras.layers import Conv2D, Flatten, Reshape\n",
    "\n",
    "from keras.layers import Conv2D, Flatten, Reshape, MaxPooling2D\n",
    "\n",
    "def createModel(size):\n",
    "    input_shape = (size, size, 4)  # Assumes square images with 4 channels\n",
    "    model = Sequential()\n",
    "    model.add(Reshape(input_shape, input_shape=(size*size*4,)))  # Reshape the flattened input data\n",
    "    model.add(Conv2D(size, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(size*size, activation='softmax', kernel_initializer='he_uniform'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = createModel(boardSize)\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n",
    "# find point of best validation accuracy\n",
    "bestValAcc = max(history.history['val_accuracy'])\n",
    "print(\"Best validation accuracy:\", round(bestValAcc,4))\n",
    "# best loss\n",
    "bestValLoss = min(history.history['val_loss'])\n",
    "print(\"Best validation loss:\", round(bestValLoss,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 100 times on state and check time\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    model.predict(np.array([state]).reshape(1, -1), verbose=0)\n",
    "end = time.time()\n",
    "print(\"Time per prediction:\", (end - start) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 100 times on state with call and check time\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    # first make state into a tensor\n",
    "    stateTensor = tf.convert_to_tensor(np.array([state]).reshape(1, -1), dtype=tf.float32)\n",
    "    model(stateTensor)\n",
    "end = time.time()\n",
    "print(\"Time per prediction with call:\", (end - start) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if both methods give same result\n",
    "stateTensor = tf.convert_to_tensor(np.array([state]).reshape(1, -1))\n",
    "print(\"Predictions equal:\", np.array_equal(model.predict(np.array([state]).reshape(1, -1), verbose=0), model(stateTensor)))\n",
    "# print prediction\n",
    "print(\"Prediction:\", model(stateTensor))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if NN mcts is better than normal mcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "iterations: 100\n",
      "Starting player won 9 times and lost 11 times\n",
      "NeuralMCTS won 15 times, lost 5 times, and drew 0 times\n",
      "MCTS won 5 times, lost 15 times, and drew 0 times\n"
     ]
    }
   ],
   "source": [
    "boardSize = 4\n",
    "bestModel = loadModel(f'model.{boardSize}')\n",
    "\n",
    "mctsPlayer = MCTSPlayer(maxIters=100, maxTime=999999999, argmax=True)\n",
    "nnMctsPlayer = NeuralMCTSPlayer(model=bestModel, maxIters=100, maxTime=999999999, argmax=True)\n",
    "tournament = Tournament(HexGame, [nnMctsPlayer, mctsPlayer], boardSize=boardSize, plot=True)\n",
    "tournament.run(10)\n",
    "tournament.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 414\n",
      "iterations: 465\n",
      "iterations: 618\n",
      "iterations: 1195\n",
      "iterations: 1525\n",
      "iterations: 368\n",
      "iterations: 399\n",
      "iterations: 507\n",
      "iterations: 870\n",
      "iterations: 1472\n",
      "iterations: 406\n",
      "iterations: 523\n",
      "iterations: 550\n",
      "iterations: 901\n",
      "iterations: 1243\n",
      "iterations: 364\n",
      "iterations: 390\n",
      "iterations: 579\n",
      "iterations: 1423\n",
      "iterations: 417\n",
      "iterations: 420\n",
      "iterations: 781\n",
      "iterations: 1345\n",
      "iterations: 366\n",
      "iterations: 408\n",
      "iterations: 574\n",
      "iterations: 1451\n",
      "iterations: 416\n",
      "iterations: 531\n",
      "iterations: 759\n",
      "iterations: 1235\n",
      "iterations: 370\n",
      "iterations: 472\n",
      "iterations: 718\n",
      "iterations: 1591\n",
      "iterations: 397\n",
      "iterations: 528\n",
      "iterations: 929\n",
      "iterations: 1022\n",
      "iterations: 1345\n",
      "iterations: 360\n",
      "iterations: 397\n",
      "iterations: 551\n",
      "iterations: 1294\n",
      "iterations: 399\n",
      "iterations: 513\n",
      "iterations: 949\n",
      "iterations: 1019\n",
      "iterations: 1359\n",
      "iterations: 361\n",
      "iterations: 392\n",
      "iterations: 482\n",
      "iterations: 972\n",
      "iterations: 1538\n",
      "iterations: 386\n",
      "iterations: 531\n",
      "iterations: 796\n",
      "iterations: 1383\n",
      "iterations: 365\n",
      "iterations: 393\n",
      "iterations: 734\n",
      "iterations: 1698\n",
      "iterations: 408\n",
      "iterations: 532\n",
      "iterations: 768\n",
      "iterations: 1323\n",
      "iterations: 369\n",
      "iterations: 412\n",
      "iterations: 550\n",
      "iterations: 1449\n",
      "iterations: 419\n",
      "iterations: 474\n",
      "iterations: 583\n",
      "iterations: 943\n",
      "iterations: 1294\n",
      "iterations: 1654\n",
      "iterations: 373\n",
      "iterations: 397\n",
      "iterations: 627\n",
      "iterations: 1352\n",
      "iterations: 408\n",
      "iterations: 517\n",
      "iterations: 554\n",
      "iterations: 848\n",
      "iterations: 1348\n",
      "iterations: 374\n",
      "iterations: 427\n",
      "iterations: 597\n",
      "iterations: 734\n",
      "iterations: 1054\n",
      "iterations: 1349\n",
      "Starting player won 10 times and lost 10 times\n",
      "NeuralNet won 0 times, lost 20 times, and drew 0 times\n",
      "MCTS won 20 times, lost 0 times, and drew 0 times\n"
     ]
    }
   ],
   "source": [
    "# test if neuralnet alone is better than mcts\n",
    "mctsPlayer = MCTSPlayer(maxIters=10000000, maxTime=1, argmax=True)\n",
    "nnPlayer = NeuralNetPlayer(model=bestModel, argmax=True)\n",
    "tournament = Tournament(HexGame, [nnPlayer, mctsPlayer], boardSize=boardSize, plot=True)\n",
    "tournament.run(10)\n",
    "tournament.printResults()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for critic\n",
    "boardSize = 4\n",
    "# Let two mcts players play against each other\n",
    "mctsPlayer1 = MCTSPlayer(maxIters=100, maxTime=1000, argmax=True)\n",
    "mctsPlayer2 = MCTSPlayer(maxIters=100, maxTime=1000, argmax=True)\n",
    "tournament = Tournament(HexGame, [mctsPlayer1, mctsPlayer2], boardSize=boardSize, plot=False)\n",
    "tournament.run(200)\n",
    "tournament.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get replay buffer from each players\n",
    "replayBuffer = mctsPlayer1.mcts.replayBuffer + mctsPlayer2.mcts.replayBuffer\n",
    "print(f'Length of replay buffer: {len(replayBuffer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [r[0] for r in replayBuffer]\n",
    "y = [r[-1] for r in replayBuffer]\n",
    "X = np.array(X).reshape(len(X), -1)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "# for all -1's in y set it to 0\n",
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# CRITIC\n",
    "def createCriticModel(size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size*size, input_dim=size*size+1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # dropout layer\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(size, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Add another Dense layer\n",
    "    model.add(Dense(size, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Add another dropout layer\n",
    "    model.add(Dropout(0.3))\n",
    "    # final layer is a singular value from 0 to 1\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='he_uniform'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback to stop training when validation accuracy has stopped improving\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max', restore_best_weights=True)\n",
    "# Model checkpoint callback to save the best model weights during training\n",
    "model_checkpoint = ModelCheckpoint('best_model_weights.h5', monitor='val_accuracy', save_best_only=True, verbose=0, mode='max')\n",
    "\n",
    "critic = createCriticModel(boardSize)\n",
    "# split into train and test\n",
    "X_train = X[:int(len(X)*0.8)]\n",
    "y_train = y[:int(len(y)*0.8)]\n",
    "X_test = X[int(len(X)*0.8):]\n",
    "y_test = y[int(len(y)*0.8):]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# train model until convergence\n",
    "history = critic.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), verbose=0, batch_size=64, callbacks=[early_stopping, model_checkpoint], shuffle=True)\n",
    "critic.load_weights('best_model_weights.h5')\n",
    "# print val accuracy and loss\n",
    "print(f'Val accuracy: {history.history[\"val_accuracy\"][-1]}')\n",
    "print(f'Val loss: {history.history[\"val_loss\"][-1]}')\n",
    "print(f'Mean of y_test: {np.mean(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy \n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### critic in real-time game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1 = MCTSPlayer(maxIters=100, maxTime=1000, argmax=True)\n",
    "player2 = MCTSPlayer(maxIters=100, maxTime=1000, argmax=True)\n",
    "\n",
    "game = HexGame(None, None, boardSize, plot=False)\n",
    "turn = 0\n",
    "while not game.isTerminal():\n",
    "    print(critic.predict(game.getNNState(), verbose=0)[0][0])\n",
    "    if turn % 2 == 0:\n",
    "        player1.playAction(game)\n",
    "    else:\n",
    "        player2.playAction(game)\n",
    "    turn += 1\n",
    "print(\"winner is: \", game.getResult())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test how good on last move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1 = MCTSPlayer(maxIters=100, maxTime=1000, argmax=True)\n",
    "player2 = MCTSPlayer(maxIters=100, maxTime=1000, argmax=True)\n",
    "\n",
    "correct = 0\n",
    "for _ in range(20):\n",
    "    game = HexGame(None, None, boardSize, plot=False)\n",
    "    turn = 0\n",
    "    while not game.isTerminal():\n",
    "        lastState = game.getNNState()\n",
    "        if turn % 2 == 0:\n",
    "            player1.playAction(game)\n",
    "        else:\n",
    "            player2.playAction(game)\n",
    "        turn += 1\n",
    "    pred = critic.predict(lastState, verbose=0)[0][0]\n",
    "    print(f'Prediction: {pred}, Actual: {game.getResult()}')\n",
    "    if pred > 0.5 and game.getResult() == 1:\n",
    "        correct += 1\n",
    "    elif pred < 0.5 and game.getResult() == -1:\n",
    "        correct += 1\n",
    "\n",
    "print(f'Accuracy: {correct/20}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential games comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 4\n",
    "# model = createModel(size=boardSize)\n",
    "model = loadModel(f'model.{boardSize}')\n",
    "neuralNetPlayer = NeuralNetPlayer(model=model, argmax=True)\n",
    "randomPlayer = RandomPlayer()\n",
    "tournament = Tournament(HexGame, neuralNetPlayer, randomPlayer, boardSize=boardSize, plot=True)\n",
    "tournament.run(rounds)\n",
    "wins, losses, draws = tournament.getResults()\n",
    "print(f\"Neuralnet Player: {wins} wins, {losses} losses, {draws} draws\")\n",
    "\n",
    "replay = nnMctsPlayer.mcts.replayBuffer\n",
    "# TODO: flip both axis and double the replay buffer\n",
    "print(f'Length of replay buffer: {len(replay)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on replay buffer\n",
    "X = np.array([x[0] for x in replay]).reshape(len(replay), boardSize*boardSize)\n",
    "y = np.array([x[1] for x in replay]).reshape(len(replay), boardSize*boardSize)\n",
    "model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new game\n",
    "game = HexGame(None, None, size=3)\n",
    "board = game.getNNState()\n",
    "# prediction = model.predict(board)\n",
    "\n",
    "# plot distribution of actions predictions of empty board\n",
    "# plt.scatter(range(len(prediction[0])), prediction[0])\n",
    "\n",
    "# plot distribution actions of empty board with mcts\n",
    "mc = Mcts(maxIters=5000, maxTime=15)\n",
    "mc.search(game)\n",
    "dist = mc.replayBuffer\n",
    "plt.scatter(range(len(dist[0][-1])), dist[0][-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check who wins by counting 1's and -1's in last layer of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5\n",
    "dataName = f'replayBuffer{s}.pickle'\n",
    "with open(dataName, 'rb') as f:\n",
    "    replay = pickle.load(f)\n",
    "\n",
    "X = np.array([x[0] for x in replay]).reshape(len(replay),-1)\n",
    "y = np.array([x[1] for x in replay]).reshape(len(replay),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel(size=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = [game.getNNState()[0] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = np.array(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load replaybuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataName = f'replayBuffer{boardSize}.pickle'\n",
    "with open(dataName, 'rb') as f:\n",
    "    replay = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x[0] for x in replay]).reshape(len(replay), boardSize*boardSize)\n",
    "y = np.array([x[1] for x in replay]).reshape(len(replay), boardSize*boardSize)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "numModels = 5\n",
    "models = []\n",
    "\n",
    "for i in range(numModels):\n",
    "    newModel = tf.keras.models.clone_model(model)\n",
    "    newModel.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.01))\n",
    "    newModel.fit(X, y, epochs=20, verbose=0)\n",
    "    models.append(newModel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test vs random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_better = 0\n",
    "total_wins = []\n",
    "for i in range(numModels):\n",
    "    tournament = Tournament(HexGame, NeuralNetPlayer(model=models[i]), RandomPlayer(), boardSize=boardSize)\n",
    "    tournament.run(11)\n",
    "    wins, losses, draws = tournament.getResults()\n",
    "    total_wins.append(wins)\n",
    "    if wins > losses:\n",
    "        nn_better += 1\n",
    "    print(f\"Model {i} vs random: {wins} wins, {losses} losses, {draws} draws\")\n",
    "\n",
    "    tournament = Tournament(HexGame, NeuralNetPlayer(model=models[i]),  MCTSPlayer(maxIters=50, maxTime=20), boardSize=boardSize)\n",
    "    tournament.run(11)\n",
    "    wins, losses, draws = tournament.getResults()\n",
    "    total_wins.append(wins)\n",
    "    if wins > losses:\n",
    "        nn_better += 1\n",
    "    print(f\"Model {i} vs mcts: {wins} wins, {losses} losses, {draws} draws\")\n",
    "\n",
    "print(f\"NN MCTS Player: {nn_better} models better than random player\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(total_wins)\n",
    "print(f'winrate: {int(100*sum(total_wins)/len(total_wins)/11)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[0]\n",
    "# use %magic to make plots pop up in a separate window\n",
    "tournament = Tournament(HexGame, NeuralNetPlayer(model=model), RandomPlayer(), boardSize=boardSize, plot=True)\n",
    "tournament.run(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e56f9ad386266bf5d5cd9b6002e19566fae980b41617c03a2c39443e844065f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
